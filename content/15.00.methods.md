## Methods

### MultiPLIER and Pathway-level information extractor (PLIER)

MultiPLIER [@doi:10.1016/j.cels.2019.04.003] extracts patterns of co-expressed genes from recount2 [@doi:10.1038/nbt.3838], a large gene expression dataset.
The approach applies the pathway-level information extractor method (PLIER) [@doi:10.1038/s41592-019-0456-1], which performs unsupervised learning using prior knowledge (cannonical pathways) to reduce technical noise.
Via a matrix factorization approach, PLIER deconvolutes the gene expression data into a set of latent variables (LV), where each represents a gene module (i.e. a set of genes with coordinated expression patterns).
This reduced the data dimensionality into 987 latent variables.

Given a gene expression dataset $\mathbf{Y}^{n \times p}$ with $n$ genes and $p$ conditions and a prior knowledge matrix $\mathbf{C} \in \{0,1\}^{n \times m}$ for $m$ gene sets (so that $\mathbf{C}_{ij} = 1$ if gene $i$ belongs to gene set $j$), (e.g., gene sets from MSigDB [@doi:10.1016/j.cels.2015.12.004]), PLIER finds $\mathbf{U}$, $\mathbf{Z}$, and $\mathbf{B}$ minimizing

$$
||\mathbf{Y} - \mathbf{Z}\mathbf{B}||^{2}_{F} + \lambda_1 ||\mathbf{Z} - \mathbf{C}\mathbf{U}||^{2}_{F} + \lambda_2 ||\mathbf{B}||^{2}_{F} + \lambda_3 ||\mathbf{U}||_{L^1}
$$ {#eq:met:plier_func}

subject to $\mathbf{U}>0, \mathbf{Z}>0$;
$\mathbf{Z}^{n \times l}$ are the gene loadings with $l$ latent variables,
$\mathbf{B}^{l \times p}$ is the latent space for $p$ conditions,
$\mathbf{U}^{m \times l}$ specifies which of the $m$ prior-information gene sets in $\mathbf{C}$ are represented for each LV,
and $\lambda_i$ are different regularization parameters used in the training step.
<!--  -->
$\mathbf{Z}$ is a low-dimensional representation of the gene space where each LV aligns as much as possible to prior knowledge and it might represent a known or novel gene module (i.e., a meaningful biological pattern) or noise.


### CRISPR-Cas9 screening

`Add details`{.red}


### Consensus clustering of traits

<!-- We used a consensus clustering approach [@Strehl2002; @doi:10.1109/TPAMI.2005.113; @doi:10.1109/TKDE.2014.2316512; @doi:10.1109/TKDE.2019.2903410] to discover groups of traits similarly affected by the same transcriptional processes. -->
<!-- It has been shown that this approach generates more robust solutions than individual clustering runs, and they more accurately represent the underlying structure of data. -->
<!--  -->
Let $\mathcal{X} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\}$ be the dataset, where each vector $\mathbf{x}_i = (x_{i1}, x_{i2}, \ldots, x_{im})^\intercal$ represents the $i$th trait, and $x_{ij}$ is a scalar for the $j$ latent variable or gene module.
A partitioning of $\mathcal{X}$ with $n$ traits into $k$ clusters is represented as a label vector $\pi \in \mathbb{N}^n$.
<!--  -->
Consensus clustering approaches consist of two steps:
1) the generation of an ensemble $\Pi$ with $r$ partitions of the dataset: $\Pi=\{\pi_1, \pi_2, \ldots, \pi_r\}$,
and 2) the combination of the ensemble into a consolidated solution defined as:

$$
\pi^* = \mathrm{arg}\,\underset{\hat{\pi}}{\max} Q(\{ \lvert \mathcal{L}^i \lvert \phi(\hat{\pi}_{\mathcal{L}^i}, \pi_{i \mathcal{L}^i}) \mid i \in \{1,\ldots,r\} \}),
$$ {#eq:consensus:obj_func}

where $\mathcal{L}^i$ is a set of data indices with known cluster labels for partition $i$,
$\phi\colon \mathbb{N}^n \times \mathbb{N}^n \to \mathbb{R}$ is a function that measures the similarity between two partitions,
and $Q$ is a measure of central tendency, such as the mean or median.
We used the adjusted Rand index (ARI) [@doi:10.1007/BF01908075] for $\phi$, and the median for $Q$.
<!--  -->
To obtain $\pi^*$, we define a consensus function $\Gamma\colon \mathbb{N}^{n \times r} \to \mathbb{N}^n$ with $\Pi$ as the input.
We used consensus functions based on the evidence accumulation clustering (EAC) paradigm [@doi:10.1109/TPAMI.2005.113], where $\Pi$ is first transformed into a pairwise co-association matrix $\mathcal{S} \in \mathbb{R}^{n \times n}$ defined as

$$
\mathcal{S}_{ij} = \frac{n_{ij}}{r},
$$ {#eq:consensus:coassoc_mat}

where $n_{ij}$ is the number of times traits $i$ and $j$ were grouped in the same cluster across all $r$ partitions in $\Pi$.
Then, $\Gamma$ can be any similarity-based clustering algorithm, which is applied on $\mathcal{S}$ to derive the final partition $\pi^*$.
<!-- We used two EAC-based consensus functions: one based on hierarchical clustering and the other based on spectral clustering.
For each $k$, we took the partition that maximized Equation @eq:consensus:obj_func. -->


`(CHECK)`{.red} In our study, the input data has $n$=4091 traits and $m$=987 latent variables.
For the ensemble generation step, we used different approaches to create a highly diverse set of partitions (see Figure @fig:clustering:design), since diversity is an important property for ensembles [@doi:10.1016/j.ins.2016.04.027; @doi:10.1109/TPAMI.2011.84; @doi:10.1016/j.patcog.2014.04.005].
We used three data representations: the raw dataset, its projection into the top 50 principal components, and the embedding learned by UMAP [@arxiv:1802.03426] using 50 components.
<!--  -->
For each of these, we applied five clustering algorithms, covering a wide range of different assumptions on the data structure: $k$-means [@Arthur2007], spectral clustering [@Ng2001], a Gaussian mixture model (GMM), hierarchical clustering, and DBSCAN [@Ester1996].
<!--  -->
For $k$-means, spectral clustering and GMM, we specified a range of $k$ between 2 and $\sqrt{n} \approx 60$, and for each $k$ we generated five partitions using random seeds.
<!--  -->
For hierarchical clustering, for each $k$ we generated four partitions using four common linkage criteria: ward, complete, average and single.
<!--  -->
For DBSCAN, we combined different ranges for parameters $\epsilon$ (the maximum distance between two data points to be considered part of the same neighborhood) and *minPts* (the minimum number of data points in a neighborhood for a data point to be considered a core point).
Specifically, we used *minPts* values from 2 to 125, and for each data version, we determined a plausible range of $\epsilon$ values by observing the distribution of the mean distance of the *minPts*-nearest neighbors across all data points.
Since some combinations of *minPts* and $\epsilon$ might not produce a meaningful partition (for instance, when all points are detected as noisy or only one cluster is found), we resampled partitions generated by DBSCAN to ensure an equal representation in the ensemble.
<!--  -->
`(CHECK)`{.red} This procedure generated a final ensemble of 4428 partitions.


Finally, we used hierarchical and spectral clustering on the co-association matrix (Equation @eq:consensus:coassoc_mat) to derive the final consensus partitions.
For each $k$ between 2 and 60, we ran these two approaches on the entire ensemble and selected the partition that maximized Equation @eq:consensus:obj_func.
<!--  -->
We further filtered this set of 59 partitions to keep only those with an agreement larger than the 75th percentile, leaving a total of 15 final consensus partitions (with $k$ from 5 to 26) shown in Figure @fig:clustering:tree.


`Notes:`{.red}

::: {style="color: red"}
- (Supplementary material?) Incorporate some figures I generated showing the partitions passing the 75th threshold.
:::


### Clusters interpretation

We used a supervised learning approach to detect which gene modules are the most important for clusters of traits.
For this, we used the highest resolution partition ($k$=26) and trained a decision tree model using each of the clusters as labels, and the projected data $\mathcal{X} \in \mathbb{R}^{n \times m}$ as the training samples.
For each $k$, labels consisted in a binary array with the current cluster as the positive class, and the rest of the traits as the negative class.
Then, we selected the LV in the root node of the trained model only if its threshold was positive and larger than one standard deviation.
Next, we removed the root node LV from $\mathcal{X}$ (regardless of being previously selected or not), and trained the model again.
This was repeated 20 times to extract at most the top 20 LVs for a particular cluster.
This procedure allowed us to easily interpret clustering results by identifing a set of potential transcriptome processes related to each cluster of traits.


### Drug-disease predictions

**NOT FINISHED**

We used the dot product of the S-PrediXcan $z$-score for each gene-disease pair, and the $z$-score for each gene-drug pair in LINCS L1000, multiplied by -1.

To obtain a drug-disease association for the gene module-mapped TWAS results, we first projected LINCS L1000 data into this latent representation using Equation (@eq:proj), thus leading to a matrix with the expression profiles of drugs mapped to latent variables.
This can be interpreted as the effects of compounds on gene modules activity.
Then, similarly as before, we anti-correlated gene module-traits scores and module expression profiles of drugs.
