### Clusters of traits in the gene module space are associated with relevant transcriptional processes

![
**Cluster analysis on traits from PhenomeXcan.**
<!--  -->
**a)** The projection of TWAS results for 3,749 traits to the latent
representation learned from recount2 are the input data to the clustering
process. A linear (PCA) and non-linear (UMAP) dimensionality reduction
techniques are applied to the input data, and the three data versions are
processed by five different clustering algorithms. These algorithms derive
partitions from the data using different sets of parameters (such as the number
of clusters), leading to an ensemble of 4,428 partitions. A coassociation matrix
is derived by counting how many times a pair of traits were grouped together in
the ensemble. Finally, a consensus function is applied to the coassociation
matrix to generate consolidated partitions with different number of clusters.
These final solutions are represented in the clustering tree (Figure
@fig:clustering:tree).
<!--  -->
**b)** The clusters found by the consensus function are used as labels to train a decision tree classifier on the original input data, which detects the most important LVs that differentiate groups of traits.
<!--  -->
](images/clustering/clustering_design.svg "Cluster analysis on
traits"){#fig:clusering:design width="100%"}


All traits in PhenomeXcan were projected into the latent space learned from
recount2 using Equation (@eq:proj). We conducted cluster analysis using this new
representation to find groups of traits that are similarly affected by the same
transcriptional processes.
<!--  -->
To avoid using a single clustering algorithm (which implies using a single
assumption about the structure of the data), we employed a consensus clustering
approach where different methods with varying sets parameters are applied on the
same data, and later combined into a consolidated solution
[@doi:10.1016/j.ins.2016.04.027; @doi:10.1109/TPAMI.2005.237; @Strehl2002]
(Figure @fig:clusering:design).
<!--  -->
An important property for a successful application of a consensus clustering
approach is the diversity of the ensemble, understood as the level of
disagreement between the base clustering solutions
[@doi:10.1016/j.ins.2016.04.027; @doi:10.1109/TPAMI.2011.84;
@doi:10.1016/j.patcog.2014.04.005].
A diverse set of solutions can be generated by using different data representations (such as dimensionality reduction methods or subsets of features), clustering algorithms with distinct assumptions ($k$-means, for instance, assumes hyperspherical clusters), and a varying set of algorithm's parameters (such as the number of clusters or the initial random seeds).
<!--  -->
In our approach, we performed cluster analysis using five different clustering
algorithms on three representations of the input data (the original data with
987 latent variables, its projection into the top 50 principal components, and
the embedding learned by UMAP [@arxiv:1802.03426] using 50 components) (see
Figure @fig:clusering:design a). The clustering methods used cover a wide range
of different assumptions on cluster shapes and a varying set of parameters such
as the number of clusters (from 2 to 60), the width of the Gaussian kernel in
spectral clustering, and other method-specific parameters (see the supplementary
material for more details).
<!--  -->
The process generated an ensemble with 4,428 clustering solutions for all
traits. This ensemble was used to derive a coassociation matrix between traits
by counting the number of times a pair of traits was clustered together.
A consensus function was applied on the coassociation matrix to derive a consolidated solution using the information in the ensemble.
For these final partitions, we did not select a specific number of clusters, but instead used a clustering tree [@doi:10.1093/gigascience/giy083] (Figure @fig:clustering:tree) to examine stable groups of traits at multiple resolutions.
<!--  -->
Finally, for the interpretation of the clusters, we trained a decision tree
classifier (a highly interpretable machine learning model) on the original
input data using the clusters found as labels. This approach allowed us to
quickly identify the most important gene modules for the groups of traits found.
<!--  -->
More details of the clustering process are available in the supplementary
material.


![
**Clustering tree using multiple resolutions.**
<!--  -->
Clustering tree of the consensus partitions at different resolutions (from 4 to 45 clusters).
Each row represents a partition of the traits, and each circle is a cluster from that partition.
Arrows indicate how traits in one cluster move across clusters from different partitions.
Some clusters, such as anthropometric traits, blood cell counts, and some complex diseases are highly stable across different resolutions.
<!--  -->
](images/clustering/clustering_tree.svg "Clustering tree using the consensus solutions for traits"){#fig:clustering:tree width="100%"}


A clustering tree of the consensus solutions at different
resolutions is shown in Figure @fig:clustering:tree. For each $k$ (the number of
clusters), the consensus partition that maximized the agreement with the
ensemble was selected (see supplementary material).
Since it is expected that a subset of resolutions better represents the patterns among traits, we further filtered the consensus partitions by taking those with an agreement value higher than the 75th percentile, which included partitions from 4 to 45 clusters.


The clustering tree shows four clear branches (from right to left):
results from hematological assays (blood cell count), anthropometric traits, bone-densitometry of heel and keratometry measurements, and the "complex" branch.
<!--  -->
The first three branches include highly stable clusters, where the same traits are clustered together across different resolutions and even different random seeds in the consensus clustering algorithm.
The complex branch also includes stable clusters at different resolutions, such as
1) skin and hair color,
2) blood pressure and hypertension,
3) asthma, allergic rhinitis, and atopic dermatitis,
4) cardiovascular diseases (coronary artery disease, myocardial infarction, angina pectoris, among others) and related medications,
5) schizophrenia, educational outcomes, and fluid intelligence score
6) autoimmune diseases (celiac disease, hypothyroidism, psoriasis, rheumatoid arthritis, systemic lupus erythematosus, type 1 diabetes, among others),
7) breath spirometry,
8) nutrients intake,
9) lipids and Alzheimer's disease,
10) intraocular pressure (cornea),
11) smoking habits, and
12) happiness and well-being.
<!--  -->
In the following sections, we selected some of these stable clusters to analyze the transcriptional processes that differentiate them.


::: {style="color: red"}
- Maybe include Ben's question about why we expect these clustering results. My answer was more or less that far apart clusters are explained by more specific LVs to those traits, and the complex branch is more related to traits that are highly connected to all biological processes, where more subtle differences are captured only at higher resolutions.
:::
