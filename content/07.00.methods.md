## Methods and materials {#sec:methods}

PhenoPLIER is a computational framework that integrates gene-trait associations, drug-induced transcriptional responses, and groups of functionally-related genes (referred to as gene modules or latent variables/LVs).
PrediXcan family of methods are used to compute gene-trait associations, while MultiPLIER models are applied on large gene expression compendia to infer latent variables.
PhenoPLIER provides a regression model to compute an LV-trait association, a consensus clustering approach applied to the latent space to learn shared and distinct transcriptomic properties between traits, and an interpretable, LV-based drug repurposing framework.
We provide details of these methods in the following section.


### The PrediXcan family of methods for gene-based associations {#sec:methods:predixcan}

We used two gene-based statistical approaches from the PrediXcan family of methods [@doi:10.1038/ng.3367], Summary-PrediXcan (S-PrediXcan) [@doi:10.1038/s41467-018-03621-1] and Summary-MultiXcan (S-MultiXcan) [@doi:10.1371/journal.pgen.1007889].
We refer to these approaches as Transcription-Wide Association Studies (TWAS).
S-PrediXcan is the summary-based version of PrediXcan and computes the univariate association between a trait and a gene's predicted expression in a single tissue.
On the other hand, S-MultiXcan is the summary-based version of MultiXcan and computes the joint association between a gene's predicted expression in all tissues and a trait.
Both of these methods require only GWAS summary statistics instead of individual-level genotype and phenotype data.

We provide a brief overview of the TWAS methods used in this study.
We refer to $\mathbf{y}$ as a vector of traits for $n$ individuals, which has been centered (no intercept is necessary).
For each tissue $l$, $\mathbf{\tilde{t}}_l$ is the gene's predicted expression for all individuals, which is determined by $X_a$ (the genotype of SNP $a$) and $w_{a}$ (its weight in the tissue prediction model $l$).
$\mathbf{t}_l$ is the standardized version of $\mathbf{\tilde{t}}_l$, where the mean is equal to zero and the standard deviation is equal to one.
For more information, please refer to the referenced articles.

.
S-PrediXcan, however, uses a multivariate model to predict the trait by fitting the expression of multiple genes simultaneously.
To obtain the gene expression data, we used the GTEx database [@doi:10.1038/nature14248].
Clustering of complex traits was performed using the k-means algorithm.

We used S-PrediXcan and the GTEx database to project genetic associations through gene expression patterns.
To do this, we used the multivariate model of S-PrediXcan to

$$
\mathbf{y} = \mathbf{t}_l \gamma_l + \bm{\epsilon}_l,
$$ {#eq:predixcan}

<!--  -->
-prediction model.

We assess the significance of the genetic association by computing the $z$-score $\hat{z}_{l}=\hat{\gamma}_l / \mathrm{se}(\hat{\gamma}_l)$ for a gene's tissue model $l$.
This involves estimating the effect size or regression coefficient $\hat{\gamma}_l$ and the variance of the error terms $\sigma_{\epsilon}^{2}$.
PrediXcan requires individual-level data to fit this model, whereas S-PrediXcan uses GWAS summary statistics and an expression-prediction model to approximate PrediXcan $z$-scores.

$$
\hat{z}_{l} \approx \sum_{a \in model_{l}} w_a^l \frac{\hat{\sigma}_a}{\hat{\sigma}_l} \frac{\hat{\beta}_a}{\mathrm{se}(\hat{\beta}_a)},
$$ {#eq:spredixcan}

<!--  -->
The TWAS methods used in this study estimate genotype variances and covariances using the Genotype-Tissue Expression project (GTEx v8) [@doi:10.1126/science.aaz1776] as the reference panel.
The estimated effect size of each SNP, $\hat{\beta}_a$, and the variance of the predicted expression of a gene in tissue $l$, $\hat{\sigma}_l$, are used to calculate the $z$-scores.
These $z$-scores are used in our drug repurposing approach (described below) to determine the direction of effects (e.g.
whether a higher or lower predicted expression of a gene confers more or less disease risk).

, which is defined as the combination of gene expression and genetic data.
S-MultiXcan is used to cluster complex traits and identify therapeutic targets, and it is also suitable for drug repurposing.

S-MultiXcan is an alternative to PrediXcan, which is more powerful in detecting gene-trait associations, though it does not indicate the direction of effects.
S-MultiXcan produces an F-test $p$-value as its main output, which is a combination of gene expression and genetic data.
It is useful for clustering complex traits, identifying therapeutic targets, and drug repurposing.

$$
\begin{split}
\mathbf{y} & = \sum_{l=1}^{p} \mathbf{t}_l g_l + \mathbf{e} \\
 & = \mathbf{T} \mathbf{g} + \mathbf{e},
\end{split}
$$ {#eq:multixcan}

<!--  -->
the $p$-value from this chi-square test.

MultiXcan uses the principal components (PCs) of a matrix $\mathbf{T}$ with $p$ columns $\mathbf{t}_l$ to avoid collinearity issues.
The estimated effect size for the predicted gene expression in tissue $l$ is denoted by $\hat{g}_l$ and $\mathbf{\hat{g}}$ is a vector with $p$ estimated effect sizes.
The joint regression estimates (effect sizes and their variances) in Equation (@eq:multixcan) are derived from the marginal estimates from S-PrediXcan in Equation (@eq:spredixcan), and the error terms have variance $\sigma_{e}^{2}$.
The significance of the association in S-MultiXcan is estimated with the $p$-value from a chi-square test, which is distributed as $\mathbf{\hat{g}}^{\top} \frac{\mathbf{T}^{\top}\mathbf{T}}{\sigma_{e}^{2}} \mathbf{\hat{g}} \sim \chi_{p}^{2}$ under the null hypothesis of no association.

$$
\begin{split}
\frac{\mathbf{\hat{g}}^{\top} (\mathbf{T}^{\top}\mathbf{T}) \mathbf{\hat{g}}}{\sigma_{e}^{2}} & \approx \bm{\hat{\gamma}}^{\top} \frac{\sqrt{n-1}}{\sigma_{\epsilon}} \left(\frac{\mathbf{T}^{\top} \mathbf{T}}{n-1}\right)^{-1} \frac{\sqrt{n-1}}{\sigma_{\epsilon}} \bm{\hat{\gamma}} \\
 & = \mathbf{\hat{z}}^{\top} Cor(\mathbf{T})^{-1} \mathbf{\hat{z}},
\end{split}
$$ {#eq:smultixcan}

<!--  -->
S-MultiXcan is used to calculate the $z$-scores for each gene in a given tissue.
It uses the top $k$ principal components (PCs) from the autocorrelation matrix of $\mathbf{T}$, and the conservative approximation $\sigma_{e}^{2} \approx \sigma_{\epsilon}^{2}$, where $\mathbf{T}$ is a vector of gene expression values.
This approximation assumes that the variance of the error terms in the joint regression is approximately equal to the residual variance of the marginal regressions.
The correlation matrix is estimated using a global genotype covariance matrix, while the marginal $z$-scores are approximated using tissue-specific genotype covariances.
Although S-MultiXcan yields highly concordant estimates compared with MultiXcan, results are not perfectly correlated across genes.
These differences were important for our LV-based regression model when computing the gene-gene correlation matrix, which was used for our cluster analyses of traits.


### TWAS resources {#sec:methods:twas}

<!--  -->
<!--  -->
<!--  -->
<!--  -->
We used two large TWAS resources with European ancestry for discovery and replication.
PhenomeXcan [@doi:10.1126/sciadv.aba2083], our discovery cohort, provides results on 4,091 traits across different categories (see Supplementary File 1 for details about the included GWAS, sample size and disease/trait categories).
We used PrediXcan and fastENLOC [@doi:10.1126/sciadv.aba2083; @doi:10.1016/j.ajhg.2020.11.012] to compute gene-based associations with the PrediXcan family of methods and a posterior probability of colocalization between GWAS loci and *cis*-eQTL.
We refer to the matrix of $z$-scores from S-PrediXcan (Equation (@eq:spredixcan)) across $q$ traits and $m$ genes in tissue $t$ as $\mathbf{M}^{t} \in \mathbb{R}^{q \times m}$.
This matrix was used in our LV-based drug repurposing framework since it provides direction of effects.
The S-MultiXcan results (22,515 gene associations across 4,091 traits) were used in our LV-based regression framework and our cluster analyses of traits.
For the cluster analyses, we converted the $p$-values to $z$-scores using the probit function: $\mathbf{M}=\Phi^{-1}(1 - p/2)$.
Higher $z$-scores correspond to stronger associations.

We used the eMERGE cohort to discover genetic associations.
This cohort included 309 phenotypes (diseases, traits, etc.) and we used the same TWAS methods to analyze them.
Further information on the traits can be found in [@doi:10.1101/2021.10.21.21265225].
We then replicated the associations found with our LV-based regression framework in PhenomeXcan.


### MultiPLIER and Pathway-level information extractor (PLIER) {#sec:methods:multiplier}

The MultiPLIER algorithm was applied to the recount2 dataset to extract patterns of co-expressed genes.
The approach used the Pathway-Level Information Extractor (PLIER) method, which performs unsupervised learning by leveraging prior knowledge (canonical pathways) to reduce technical noise.
PLIER employed a matrix factorization approach to deconvolute gene expression data into latent variables (LVs).
This approach reduced the dimensionality of the recount2 dataset to 987 LVs.

the following objective function:

We used the PLIER algorithm to project genetic associations onto gene expression patterns.
PLIER takes as input a gene expression dataset ($\mathbf{Y}^{m \times c}$) with $m$ genes and $c$ experimental conditions, and a prior knowledge matrix ($\mathbf{C} \in \{0,1\}^{m \times p}$) for $p$ MSigDB pathways [@doi:10.1016/j.cels.2015.12.004], where $\mathbf{C}_{ij} = 1$ if gene $i$ belongs to pathway $j$.
It then finds the matrices $\mathbf{U}$, $\mathbf{Z}$, and $\mathbf{B}$ that minimize the following objective function:

$$
||\mathbf{Y} - \mathbf{Z}\mathbf{B}||^{2}_{F} + \lambda_1 ||\mathbf{Z} - \mathbf{C}\mathbf{U}||^{2}_{F} + \lambda_2 ||\mathbf{B}||^{2}_{F} + \lambda_3 ||\mathbf{U}||_{L^1}
$$ {#eq:met:plier_func}

<!--  -->
We used a regularized factor analysis approach to project genetic associations through gene expression patterns.
This approach uses matrices $\mathbf{Z}$, $\mathbf{B}$, $\mathbf{U}$, and $\lambda_i$ to obtain a low-dimensional representation of the gene space.
The matrix $\mathbf{Z}$ is composed of gene loadings, $\mathbf{B}$ is the latent space for conditions, and $\mathbf{U}$ defines which prior-information pathways in $\mathbf{C}$ are represented for each latent variable.
The regularization parameters $\lambda_i$ are used in the training step.
This representation of the gene space can either represent a known or novel gene module (i.e., a meaningful biological pattern) or noise.

<!--  -->
the following equation:

We used a model to project gene-trait and gene-drug associations into a low-dimensional gene module space for drug repurposing and cluster analyses.
For example, we projected TWAS associations (either from S-PrediXcan or S-MultiXcan) using the following equation: $\mathbf{M}$

$$
\hat{\mathbf{M}} = (\mathbf{Z}^{\top} \mathbf{Z} + \lambda_{2} \mathbf{I})^{-1} \mathbf{Z}^{\top} \mathbf{M},
$$ {#eq:proj}

<!--  -->
We used a matrix $\hat{\mathbf{M}}^{l \times q}$ to represent traits with gene modules instead of individual genes.
We applied the same approach to project the transcriptional profiles of drugs from the LINCS L1000 dataset and create a representation of drugs using gene modules.


### Regression model for LV-trait associations {#sec:methods:reg}

for each LV, and tested the association of the LV with the phenotype.

We adapted the gene-set analysis framework from MAGMA to TWAS.
We used a competitive test to predict gene-trait associations from TWAS.
This test examined whether the genes with the highest weights from an LV were more strongly associated with the phenotype than other genes with small or zero weights.
We then fit the model for each LV and tested the association of the LV with the phenotype.

$$
\mathbf{m}=\beta_{0} + \mathbf{s} \beta_{s} + \sum_{i} \mathbf{x}_{i} \beta_{i} + \bm{\epsilon},
$$ {#eq:reg:model}

We model the gene $p$-values ($\mathbf{m}$) for a trait as a function of the top 1% of genes with the largest loadings for each latent variable ($\mathbf{s}$), a gene property used as a covariate ($\mathbf{x}_{i}$), and effect sizes ($\beta$).
The error terms ($\bm{\epsilon}$) are assumed to follow a multivariate normal distribution (MVN) with a mean of zero and a variance-covariance matrix of gene correlations ($\mathbf{R}$).

We tested the null hypothesis that the difference in trait associations between genes in LV $\ell$ and genes outside of it is zero against the one-sided hypothesis that the difference is greater than zero.
We used two gene properties as covariates, as suggested by the MAGMA framework: the number of PCs retained in S-MultiXcan (gene size) and the ratio of the number of PCs to the number of tissues available (gene density).

We used a generalized least squares approach to account for correlations between error terms.
To approximate the gene-gene correlation matrix $\mathbf{R}$, we computed the correlation between the model sum of squares (SSM) for each pair of genes under the null hypothesis of no association.
This correlation was derived from the individual-level MultiXcan model, which projects the predicted expression matrix $\mathbf{T}_{i} \in \mathbb{R}^{n \times p_i}$ of a gene $i$ across $p_i$ tissues into its top $k_i$ PCs, resulting in matrix $\mathbf{P}_{i} \in \mathbb{R}^{n \times k_i}$.
The SSM for each gene is proportional to $\mathbf{y}^{\top} \mathbf{P}_{i} \mathbf{P}_{i}^{\top} \mathbf{y}$.
Under the null hypothesis of no association, the covariances between the SSM of genes $i$ and $j$ is given by $2 \times \mathrm{Trace}(\mathbf{P}_{i}^{\top} \mathbf{P}_{j} \mathbf{P}_{j}^{\top} \mathbf{P}_{i})$.
The standard deviations of each SSM are $\sqrt{2 \times k_{i}} \times (n - 1)$, and the correlation between the SSMs for genes $i$ and $j$ can be written as follows:

$$
\begin{split}
\mathbf{R}_{ij} & = \frac{2 \times \mathrm{Tr}(\mathbf{P}_{i}^{\top} \mathbf{P}_{j} \mathbf{P}_{j}^{\top} \mathbf{P}_{i})}{\sqrt{2 \times k_{i}} \times \sqrt{2 \times k_{j}} \times (n - 1)^2} \\
& = \frac{2 \times \mathrm{Tr}(Cor(\mathbf{P}_{i}, \mathbf{P}_{j}) \times Cor(\mathbf{P}_{j}, \mathbf{P}_{i}))}{\sqrt{2 \times k_{i}} \times \sqrt{2 \times k_{j}}},
\end{split}
$$ {#eq:reg:r}

The columns of $\mathbf{P}$ were standardized.
The trace of a matrix, $\mathrm{Tr}$, was used to calculate the cross-correlation matrix between the principal components, $Cor(\mathbf{P}_{i}, \mathbf{P}_{j}) \in \mathbb{R}^{k_i \times k_j}$.

$$
\begin{split}
Cor(\mathbf{P}_{i}, \mathbf{P}_{j}) & = Cor(\mathbf{T}_{i} \mathbf{V}_{i}^{\top} \mathrm{diag}(\lambda_i)^{-1/2}, \mathbf{T}_{j} \mathbf{V}_{j}^{\top} \mathrm{diag}(\lambda_j)^{-1/2}) \\
& = \mathrm{diag}(\lambda_i)^{-1/2} \mathbf{V}_{i} (\frac{\mathbf{T}_{i}^{\top} \mathbf{T}_{j}}{n-1}) \mathbf{V}_{j}^{\top} \mathrm{diag}(\lambda_j)^{-1/2},
\end{split}
$$ {#eq:reg:cor_pp}

We estimated the correlation between the predicted expression levels of genes $i$ in tissue $k$ and gene $j$ in tissue $l$ by calculating the cross-correlation matrix between the predicted expression levels of genes $i$ and $j$.
This matrix was represented by $\frac{\mathbf{T}_{i}^{\top} \mathbf{T}_{j}}{n-1} \in \mathbb{R}^{p_i \times p_j}$, where $\mathbf{T}_{i}$ and $\mathbf{T}_{j}$ are the predicted expression levels of genes $i$ and $j$, respectively.
To select the top eigenvectors, we used a condition number threshold of $\frac{\max(\lambda_i)}{\lambda_i} < 30$, where $\mathbf{V}_{i}$ and $\lambda_i$ are the eigenvectors and eigenvalues of $\mathbf{T}_{i}$.
This method was described in [@doi:10.1371/journal.pgen.1007889].

$$
\begin{split}
\frac{(\mathbf{T}_{i}^{\top} \mathbf{T}_{j})_{kl}}{n-1} & = Cor(\mathbf{t}_k^i, \mathbf{t}_l^j) \\
 & = \frac{ Cov(\mathbf{t}_k, \mathbf{t}_l) } { \sqrt{\widehat{\mathrm{var}}(\mathbf{t}_k) \widehat{\mathrm{var}}(\mathbf{t}_l)} } \\
 & = \frac{ Cov(\sum_{a \in \mathrm{model}_k} w_a^k X_a, \sum_{b \in \mathrm{model}_l} w_b^l X_b) }  {\sqrt{\widehat{\mathrm{var}}(\mathbf{t}_k) \widehat{\mathrm{var}}(\mathbf{t}_l)} } \\
 & = \frac{ \sum_{a \in \mathrm{model}_k \\ b \in \mathrm{model}_l} w_a^k w_b^l Cov(X_a, X_b)} {\sqrt{\widehat{\mathrm{var}}(\mathbf{t}_k) \widehat{\mathrm{var}}(\mathbf{t}_l)} } \\
 & = \frac{ \sum_{a \in \mathrm{model}_k \\ b \in \mathrm{model}_l} w_a^k w_b^l \Gamma_{ab}} {\sqrt{\widehat{\mathrm{var}}(\mathbf{t}_k) \widehat{\mathrm{var}}(\mathbf{t}_l)} },
\end{split}
$$ {#eq:reg:corr_genes}

We used the genotype data from the GTEx v8 reference panel to construct the genotype covariance matrix $\Gamma$.
For each single nucleotide polymorphism (SNP) $a$, we calculated the weight $w_a^k$ for gene expression prediction in the tissue model $k$.
The variance of the predicted expression values of gene $i$ in tissue $k$ was estimated using the formula from [@doi:10.1038/s41467-018-03621-1], where $X_a$ is the genotype of SNP $a$.
This formula is the same one used in all the tissue-weighted association study (TWAS) methods described in this paper.

$$
\begin{split}
\widehat{\mathrm{var}}(\mathbf{t}_k^i) & = (\mathbf{W}^k)^\top \Gamma^k \mathbf{W}^k \\
 & = \sum_{a \in \mathrm{model}_k \\ b \in \mathrm{model}_k} w_a^k w_b^k \Gamma_{ab}^k.
\end{split}
$$ {#eq:reg:var_gene}

We used the MultiXcan regression model (Equation (@eq:multixcan)) to approximate gene correlations in S-MultiXcan.
Since S-MultiXcan approximates the joint regression parameters in MultiXcan using the marginal regression estimates from S-PrediXcan (Equation (@eq:spredixcan)) with some simplifying assumptions and different genotype covariance matrices, we used a submatrix $\mathbf{R}_{\ell}$ of genes that are part of LV $\ell$ only (top 1% of genes) instead of the entire matrix $\mathbf{R}$.
This was a conservative approach since correlations are accounted for top genes only.
Our simulations ([Supplementary Note 1](#sm:reg:null_sim)) showed that the model was approximately well-calibrated and could correct for LVs with adjacent and highly correlated genes at the top (e.g., Figure @fig:reg:nulls:qqplot:lv234).
The model was also able to detect LVs associated with relevant traits (Figure @fig:lv246 and Table @tbl:sup:phenomexcan_assocs:lv246), which were replicated in a different cohort (Table @tbl:sup:emerge_assocs:lv246).

In order to obtain more accurate correlation estimates, we only considered tissue models present in S-PrediXcan results and SNPs present in GWAS used as input for the TWAS approaches in Equation (@eq:reg:corr_genes).
We computed different correlation matrices for PhenomeXcan and eMERGE.
For PhenomeXcan, most of the GWAS (4,049) were obtained from the UK Biobank using the same pipeline and including the same set of SNPs, so a single correlation matrix was used.
For the remaining GWAS, we used a single correlation matrix for each group of traits that shared the same or most of the SNPs.

We ran our regression model for all 987 LVs across the 4,091 traits in PhenomeXcan.
To replicate the results, we applied the same model to the 309 phecodes in eMERGE.
We then adjusted the $p$-values using the Benjamini-Hochberg procedure.


### LV-based drug repurposing approach {#sec:methods:drug}

We compared our LV-based method with a drug repositioning framework previously used for psychiatric traits [@doi:10.1038/nn.4618], which is based on single genes associated with a trait.
This method computes a drug-disease score by multiplying a set of signed $z$-scores from tissue $t$, $\mathbf{M}^t$, with another set of signed $z$-scores from transcriptional responses profiled in LINCS L1000 [@doi:10.1016/j.cell.2017.10.049], $\mathbf{L}^{c \times m}$ (for $c$ compounds).
$\mathbf{M}^t$ contains information on whether a higher or lower predicted expression of a gene is associated with disease risk, and $\mathbf{L}$ indicates whether a drug increases or decreases the expression of a gene.
This product is $\mathbf{D}^{t,k}=-1 \cdot \mathbf{M}^{t,k} \mathbf{L}^\top$, where $k$ refers to the number of most significant gene associations in $\mathbf{M}^t$ for each trait.
We tested $k$ values of all genes and the top 50, 100, 250, and 500.
We then averaged the score ranks across all $k$ and obtained $\mathbf{D}^t$.
Finally, for each drug-disease pair, we took the maximum prediction score across all tissues: $\mathbf{D}_{ij} = \max \{ \mathbf{D}_{ij}^t \mid \forall t \}$.


<!--  -->
For the Latent Variable (LV)-based approach, we projected the gene module matrix $\mathbf{M}^{t}$ and the gene-level matrix $\mathbf{L}$ into the gene module latent space using Equation (@eq:proj).
This resulted in $\hat{\mathbf{M}}^t$ and $\hat{\mathbf{L}}^{l \times c}$.
Finally, we calculated $\mathbf{D}^{t,k}=-1 \cdot \hat{\mathbf{L}}^{\top} \hat{\mathbf{M}}^{t,k}$, where $k$ could be all LVs or the top 5, 10, 25 and 50 (since there were an order of magnitude fewer LVs than genes).


We then used gene co-expression network analysis to cluster the traits into disease-related modules, which were used to project genetic associations to the drug-disease space.

To map PhenomeXcan traits to the Disease Ontology, we used the Experimental Factor Ontology and a GitHub repository.
We then used gene co-expression network analysis to group the traits into disease-related modules.
These modules were used to project genetic associations to the drug-disease space, which is the gold standard of drug-disease medical indications.


### Consensus clustering of traits {#sec:methods:clustering}

We performed two preprocessing steps on the S-MultiXcan results before the cluster analysis.
First, we combined results for traits that mapped to the same Experimental Factor Ontology (EFO) term using the Stouffer's method.
This involved converting the $p$-values to $z$-scores, and weighting them based on the GWAS sample size for each trait.
Second, we divided all $z$-scores for each trait by their sum to reduce the effect of highly polygenic traits.
Finally, we projected this data matrix using Equation (@eq:proj), obtaining $\hat{\mathbf{M}}$ with 3,752 traits and 987 Latent Variables (LVs) as the input of our clustering pipeline.


<!--  -->
$\hat{\pi}=\underset{\pi \in \Pi}{\operatorname{argmax}} \sum_{i=1}^n \sum_{j=1}^r \delta(\pi_j(i),\pi(i))$, where $\delta$ is the Kronecker delta.

We partitioned $\hat{\mathbf{M}}$ with $n$ traits into $k$ clusters, represented as a label vector $\pi \in \mathbb{N}^n$.
To do this, we used a consensus clustering approach which involves two steps: (1) generating an ensemble $\Pi$ with $r$ partitions of the dataset ($\Pi=\{\pi_1, \pi_2, \ldots, \pi_r\}$), and (2) combining the ensemble into a consolidated solution defined

$$
\pi^* = \mathrm{arg}\,\underset{\hat{\pi}}{\max} Q(\{ \lvert \mathcal{L}^i \lvert \phi(\hat{\pi}_{\mathcal{L}^i}, \pi_{i \mathcal{L}^i}) \mid i \in \{1,\ldots,r\} \}),
$$ {#eq:consensus:obj_func}

<!--  -->
<!--  -->
<!-- $\mathbf{D}_{ij} = \frac{d_{ij}}{r}$, -->
$\mathbf{D}_{ij} = d_{ij} / r$,
<!--  -->
where $d_{ij}$ is the number of times traits $i$ and $j$ were grouped in different clusters across all $r$ partitions in $\Pi$.
Then, $\Gamma$ can be any similarity-based clustering algorithm, which is applied on $\mathbf{D}$ to derive the final partition $\pi^*$.
and then clustered using hierarchical clustering.

We used the Adjusted Rand Index (ARI) [@doi:10.1007/BF01908075] for measuring the similarity between two partitions, and the median as a measure of central tendency.
To obtain $\pi^*$, we used consensus functions based on the Evidence Accumulation Clustering (EAC) paradigm [@doi:10.1109/TPAMI.2005.113], where $\Pi$ is first transformed into a distance matrix and then clustered using hierarchical clustering.
$\mathcal{L}^i$ is a set of data indices with known cluster labels for the partition $i$.


<!--  -->
<!--  -->
<!--  -->
<!--  -->
<!--  -->
We used a range of algorithms to generate a highly diverse set of partitions (Figure @fig:clustering:design), since diversity is important for ensembles [@doi:10.1016/j.ins.2016.04.027; @doi:10.1109/TPAMI.2011.84; @doi:10.1016/j.patcog.2014.04.005].
We used three data representations: raw data, its projection into the top 50 principal components, and the embedding learned by UMAP [@arxiv:1802.03426] using 50 components.
We applied five clustering algorithms to cover a wide range of different assumptions about the data structure: $k$-means [@Arthur2007], spectral clustering [@Ng2001], Gaussian mixture model (GMM), hierarchical clustering, and DBSCAN [@Ester1996].
For $k$-means, spectral clustering and GMM, we specified a range of $k$ from 2 to $\sqrt{n} \approx 60$, and randomly generated five partitions for each $k$.
For hierarchical clustering, we generated four partitions using common linkage criteria (ward, complete, average and single) for each $k$.
For DBSCAN, we combined different ranges for parameters $\epsilon$ (maximum distance between two data points to be considered part of the same neighborhood) and *minPts* (minimum number of data points in a neighborhood for a data point to be considered a core point), based on the procedure in [@doi:10.1088/1755-1315/31/1/012012].
We used *minPts* values from 2 to 125 and determined a plausible range of $\epsilon$ values by observing the distribution of the mean distance of the *minPts*-nearest neighbors across all data points.
We resampled partitions generated by DBSCAN to ensure an equal representation of this algorithm in the ensemble, resulting in a final ensemble of 4,428 partitions of 3,752 traits.


<!--  -->
We used spectral clustering on $\mathbf{D}$ to derive the final consensus partitions.
We first transformed $\mathbf{D}$ into a similarity matrix by applying an RBF kernel $\mathrm{exp}(-\gamma \mathbf{D}^2)$ with four different values for $\gamma$ that we determined empirically.
For each $k$ between 2 and 60, we derived four consensus partitions and selected the one that maximized Equation (@eq:consensus:obj_func).
We filtered this set of 59 solutions to keep only those with an ensemble agreement larger than the 75th percentile (Supplementary Figure @fig:sup:clustering:agreement), resulting in 15 final consensus partitions shown in Figure @fig:clustering:tree.

<!-- Clustering interpretation -->
<!--  -->
Our clustering pipeline consists of several linear and nonlinear transformations, including PCA, UMAP, and the EAC paradigm (distance matrix $\mathbf{D}$).
We applied consensus clustering to the transformed data, which has been found to be beneficial for biological data [@pmid:27303057].
To determine which gene modules/LVs are most important for each cluster of traits, we used a supervised learning approach (Figure {@fig:clustering:design}b).
We trained a decision tree model using the projected data $\hat{\mathbf{M}}$ as the training samples and the highest resolution partition ($k$=29) to create a set of binary labels with the current cluster's traits as the positive class and the rest of the traits as the negative class.
We selected the LV in the root node of the trained model if its threshold was positive and larger than one standard deviation.
We then removed this LV from $\hat{\mathbf{M}}$ and trained the model again, repeating this process 20 times to extract the top 20 LVs that best discriminate traits in a cluster from the rest.

We assessed the validity of our results by performing several analyses under a null hypothesis of no structure in the data.
This was done in Supplementary Note 2 and showed that the clustering results detected by our pipeline were real.


### CRISPR-Cas9 screening {#sec:methods:crispr}

Cell culture was performed using HepG2 cells obtained from ATCC (ATCC® HB-8065™).
Cells were maintained in Eagle's Minimum Essential Medium with L-Glutamine (EMEM, Cat.
112-018-101, Quality Biology) supplemented with 10% Fetal Bovine Serum (FBS, Gibco, Cat.16000-044), and 1% Pen/Strep (Gibco, Cat.15140-122).
They were kept in a humidity-controlled incubator at 37oC with 5% CO2, and the density of the cells was kept below 80% confluency.

The 3rd generation Broad GPP Human Brunello CRISPR knockout Pooled library (Cat.
73179-LV) provided by Addgene was used to transduce HepG2 cells.
This library consists of 76,441 sgRNAs targeting 19,114 genes in the human genome, with an average of 4 sgRNAs per gene.
The sgRNA cassettes were inserted into the lentiCRIS-PRv2 backbone between the U6 promoter and gRNA scaffold.
Lentiviral vectors encoding Cas9 were used to deliver the sgRNA cassette-containing plasmids into cells during replication.
Unsuccessful transduced cells were excluded through puromycin selection.

No-spin lentiviral transduction was used for the screen.
Cells were seeded in 6-well plates coated with Collagen-I in the presence of 8ug/ml polybrene.
Different volumes of virus (0, 50, 100, 200, 250, and 400ul) were assigned to each well, with EMEM complete media added to make the final volume of 1.24ml.
16-18hrs post-transduction, the virus/polybrene-containing media was removed and the cells were washed twice with 1x DPBS and replaced with fresh EMEM.
At 24h, cells in each well were trypsinized, diluted (e.g.,1:10), and seeded in pairs of wells of 6-well plates.
At 60hr post-transduction, cell media in each well was replaced with fresh EMEM, and 2ug/ml of puromycin was added to one well out of the pair.
2-5 days after puromycin selection, or the 0 virus well treated with puromycin had no survival of cells, cells in both wells with/without puromycin were collected and counted for viability.
The transduction efficiency was calculated by comparing the cell numbers with/without puromycin selection within each pair.
When the transduction efficiency was between 30-50%, which corresponds to an MOI of ~0.35-0.70, a volume of virus (120ul) was chosen for further large-scale viral transduction.

To ensure a coverage of 500 cells per sgRNA and 95% of cells receiving only one viral particle, ~200 million cells were seeded in 14 6-well plates with 8ug/ml of polybrene.
The virus was added to each experimental well at a MOI of 0.3-0.4.
18 hours later, the virus/PB mix medium was removed and the cells were collected, counted, and pooled into T175 flasks.
60 hours after transduction, 2ug/ml of puromycin was added to each flask and the medium was changed every two days with fresh EMEM and 2ug/ml puromycin.
After seven days of puromycin selection, the cells were collected, pooled, counted, and replated.

Cells were assigned to two groups 9 days after puromycin selection.
Approximately 200 million cells were kept in 100 mm dishes and stained with LipidSpotTM 488 (Biotium, Cat.
70065-T).
The dye was diluted to 1:100 with DPBS, and 4 mL of the staining solution was used for each dish.
The cells were incubated at 37°C for 30 minutes, and then their images were captured through a fluorescent microscope (EVOS for GFP signal detection; Figure @fig:sup:crispr:fig1).
Additionally, 20-30 million cells were collected as an unsorted control and kept at -80°C for further genomic DNA isolation.

Cells were collected in 50ml tubes and spun at 500 x g for 5 minutes at 4°C.
After a DPBS wash, cell pellets were resuspended with a FACS Sorting Buffer (1x DPBS without Ca2+/Mg2+, 2.5mM EDTA, 25mM HEPES, 1% BSA, filtered and kept at 4°C).
The cell solution was then filtered through a cell strainer (Falcon, Cat.
352235) and kept on ice, protected from light.
Using FACSJazz, cells were sorted with a 100um nozzle, with ~20% of each GFP-High and GFP-Low (Figure @fig:sup:crispr:fig2) collected into 15ml tubes.
After sorting, cells were immediately spun down and pellets were kept at -80°C for further genomic DNA isolation.

Genomic DNA was isolated from three conditions (Un-Sorted Control, lentiV2 GFP-High, and lentiV2 GFP-Low) using the QIAamp DNA Blood Mini Kit (Qiagen, Cat.51104).
The quality and quantity of the gDNA was assessed using UV Spectroscopy (Nanodrop).
Approximately 80-160ug of gDNA was isolated for each condition.
To verify the presence of the sgRNA cassette and lentiviral specific transgene, PCR was performed (Figure @fig:sup:crispr:fig3).

Illumina libraries were generated and sequenced for this project.
Primers containing a sgRNA cassette were amplified using P5/P7 primers, as indicated in [@pmid:26780180].
This was adapted from the Broad Institute protocol (Figure @fig:sup:crispr:table1).
Stagger sequence (0-8nt) was included in P5 and 8bp uniquely barcoded sequence in P7.
Primers were synthesized by Integrated DNA Technologies (IDT).
Each primer was PAGE purified and 32 PCR reactions were set up for each condition.
A 100ul PCR reaction consisted of roughly 5ug of gDNA and 5ul of each 10uM P5 and P7.
TaKaRa's ExTaq DNA Polymerase (Cat.
RR001A) was used to amplify the amplicon.
PCR Thermal Cycler Parameters were set as Initial at 95oC for 1min; 24 cycles of Denaturation at 94oC for 30 seconds, Annealing at 52.5oC for 30 seconds, Extension at 72oC for 30 seconds; and a final Elongation at 72oC for 10 minutes.
285bp-293bp PCR products were expected (Figure @fig:sup:crispr:fig4 A).
PCR products within the same condition were pooled and purified using SPRIselect beads (Beckman Coulter, Cat.
B23318).
The libraries were quantitated on Qubit and the quality was analyzed on Bio-analyzer using High Sensitivity DNA Chip.
A single approximate 285bp peak was expected (Figure @fig:sup:crispr:fig4 B).
Finally, the Illumina library samples were sequenced on Nova-seq 6000.
Samples were pooled and loaded on an SP flow cell, along with a 20% PhiX control v3 library spike-in.


### Code and data availability

We used functional genomics to identify gene co-expression patterns associated with disease etiology and drug mechanisms.
Clustering of complex traits was performed using the Phenoplier method.
We used this method to identify therapeutic targets and to explore the potential of drug repurposing. 

The code and data to reproduce all the analyses in this work are available in [https://github.com/
