## Methods

### PhenomeXcan: gene-based associations on 4,091 traits

We used TWAS results from PhenomeXcan [@doi:10.1126/sciadv.aba2083] on 4,091 traits for 22,515 genes.
PhenomeXcan was built using publicly available GWAS summary statistics to compute
1) gene-based associations with the PrediXcan family of methods [@doi:10.1038/ng.3367; @doi:10.1038/s41467-018-03621-1; @doi:10.1371/journal.pgen.1007889], and
2) a posterior probability of colocalization between GWAS loci and *cis*-eQTL with fastENLOC [@doi:10.1126/sciadv.aba2083; @doi:10.1101/2020.07.01.182097].
The PrediXcan family of methods first builds prediction models using data from the Genotype-Tissue Expression project (GTEx v8) [@doi:10.1126/science.aaz1776] for gene expression imputation and then correlate this predicted expression with the phenotype of interest.
This family is comprised of
S-PrediXcan [@doi:10.1038/s41467-018-03621-1] (which computes a gene-tissue-trait association using GWAS as input)
and S-MultiXcan [@doi:10.1371/journal.pgen.1007889] (which computes a gene-trait association by aggregating evidence of associations across all tissues).


We refer to the standardized effect sizes ($z$-scores) of S-PrediXcan across $n$ traits and $m$ genes in tissue $t$ as $\mathbf{M}^{t} \in \mathbb{R}^{n \times m}$.
For S-MultiXcan, we do not have the direction of effect, and we used the $p$-values converted to $z$-scores $\mathbf{M}=\Phi^{-1}(1 - p/2)$, where $\Phi^{-1}$ is the probit function.
Higher $z$-scores correspond to stronger associations.


### MultiPLIER and Pathway-level information extractor (PLIER)

MultiPLIER [@doi:10.1016/j.cels.2019.04.003] extracts patterns of co-expressed genes from recount2 [@doi:10.1038/nbt.3838], a large gene expression dataset.
The approach applies the pathway-level information extractor method (PLIER) [@doi:10.1038/s41592-019-0456-1], which performs unsupervised learning using prior knowledge (canonical pathways) to reduce technical noise.
Via a matrix factorization approach, PLIER deconvolutes the gene expression data into a set of latent variables (LV), where each represents a gene module.
This reduced the data dimensionality into 987 latent variables or gene modules.


Given a gene expression dataset $\mathbf{Y}^{m \times c}$ with $m$ genes and $c$ experimental conditions and a prior knowledge matrix $\mathbf{C} \in \{0,1\}^{m \times p}$ for $p$ MSigDB pathways [@doi:10.1016/j.cels.2015.12.004] (so that $\mathbf{C}_{ij} = 1$ if gene $i$ belongs to pathway $j$), PLIER finds $\mathbf{U}$, $\mathbf{Z}$, and $\mathbf{B}$ minimizing

$$
||\mathbf{Y} - \mathbf{Z}\mathbf{B}||^{2}_{F} + \lambda_1 ||\mathbf{Z} - \mathbf{C}\mathbf{U}||^{2}_{F} + \lambda_2 ||\mathbf{B}||^{2}_{F} + \lambda_3 ||\mathbf{U}||_{L^1}
$$ {#eq:met:plier_func}

subject to $\mathbf{U}>0, \mathbf{Z}>0$;
$\mathbf{Z}^{m \times l}$ are the gene loadings with $l$ latent variables,
$\mathbf{B}^{l \times c}$ is the latent space for $c$ conditions,
$\mathbf{U}^{p \times l}$ specifies which of the $p$ prior-information pathways in $\mathbf{C}$ are represented for each LV,
and $\lambda_i$ are different regularization parameters used in the training step.
<!--  -->
$\mathbf{Z}$ is a low-dimensional representation of the gene space where each LV aligns as much as possible to prior knowledge, and it might represent either a known or novel gene module (i.e., a meaningful biological pattern) or noise.


We projected $\mathbf{M}$ (either from S-PrediXcan across each tissue, or S-MultiXcan) into the low-dimensional gene module space learned by MultiPLIER using

$$
\hat{\mathbf{M}} = (\mathbf{Z}^{\top} \mathbf{Z} + \lambda_{2} \mathbf{I})^{-1} \mathbf{Z}^{\top} \mathbf{M},
$$ {#eq:proj}

where in $\hat{\mathbf{M}}^{l \times n}$ all traits in PhenomeXcan are now described by gene modules.


### LV-trait associations via gene-property analysis

To compute an association between a gene module and a trait, we used an approach similar to the gene-property analysis in MAGMA [@doi:10.1371/journal.pcbi.1004219],
which is essentially a competitive test using gene weights from $\mathbf{Z}$ to predict gene $z$-scores from $\mathbf{M}$.
Thus, the regression model uses genes as data points by fitting $\mathbf{m}=\beta_0 + \mathbf{z} \beta_z + \epsilon$, where $\epsilon \sim \mathrm{MVN}(0, \hat{\Sigma})$, $\mathbf{m}$ are gene $p$-values (for a trait) from S-MultiXcan that we transformed to $z$-scores as mentioned before.
Since we are only interested in whether genes with a stronger membership to a module (highest weights) are more associated with the phenotype, we performed a one-sided test on the coefficient $\beta_z$ with the null hypothesis of $\beta_z = 0$ against the alternative $\beta_z>0$.
Since the error terms $\epsilon$ could be correlated due to correlation between predicted expression, we used a generalized least squares approach instead of standard linear regression.
To calculate $\hat\Sigma$, we first estimated the correlation of predicted expression for each gene pair $(\mathbf{t}_i, \mathbf{t}_j)$ in tissue $t$ using equations from [@doi:10.1371/journal.pgen.1007889; @doi:10.1038/s41467-018-03621-1]:

$$
\begin{split}
\hat{\Sigma}_{ij}^{t} & = Cor(\mathbf{t}_i, \mathbf{t}_j) \\
 & = \frac{ Cov(\mathbf{t}_i, \mathbf{t}_j) } { \sqrt{\widehat{\mathrm{var}}(\mathbf{t}_i) \widehat{\mathrm{var}}(\mathbf{t}_j)} } \\
 & = \frac{ Cov(\sum_{a \in \mathrm{model}_i} w_a^i X_a, \sum_{b \in \mathrm{model}_j} w_b^j X_b) }  {\sqrt{\widehat{\mathrm{var}}(\mathbf{t}_i) \widehat{\mathrm{var}}(\mathbf{t}_j)} } \\
 & = \frac{ \sum_{a \in \mathrm{model}_i \\ b \in \mathrm{model}_j} w_a^i w_b^j Cov(X_a, X_b)} {\sqrt{\widehat{\mathrm{var}}(\mathbf{t}_i) \widehat{\mathrm{var}}(\mathbf{t}_j)} } \\
 & = \frac{ \sum_{a \in \mathrm{model}_i \\ b \in \mathrm{model}_j} w_a^i w_b^j \Gamma_{ab}} {\sqrt{\widehat{\mathrm{var}}(\mathbf{t}_i) \widehat{\mathrm{var}}(\mathbf{t}_j)} },
\end{split}
$$ {#eq:gene_corr}

where $\Gamma = \widehat{\mathrm{var}}(\mathbf{X}) = (\mathbf{X} - \mathbf{\bar{X}})^{\top} (\mathbf{X} - \mathbf{\bar{X}}) / (m-1)$ is the genotype covariance matrix using 1000 Genomes Project data [@doi:10.1038/nature15393; @doi:10.5281/zenodo.3657902].
The variances for predicted gene expression of gene $i$ is estimated as:

$$
\begin{split}
\widehat{\mathrm{var}}(\mathbf{t}_i) & = (\mathbf{W}^i)^\top \Gamma^i \mathbf{W}^i \\
 & = \sum_{a \in \mathrm{model}_i \\ b \in \mathrm{model}_i} w_a^i w_b^i \Gamma_{ab}^i.
\end{split}
$$ {#eq:gene_var}

Finally, $\hat{\Sigma} = \sum_t \hat{\Sigma}^t / |t|$ where $|t|$=49 is the number of tissues.

Because of computational reasons, we did not run the gene-property analysis on all possible LV-trait pairs.
In PhenomeXcan, we reduced the number of LV-trait pairs by considering only the top discriminative LVs for each cluster (see "Cluster interpretation" section in Methods) and the traits in that cluster, leading to 5,782 LV-trait tests.
For replication in eMERGE, we selected all the 25 LVs analyzed in the main text and ran the gene-property analysis against all 309 traits in this cohort, leading to 7,725 tests.
We adjusted the $p$-values using the Benjamini-Hochberg procedure.


### Drug-disease prediction

For the drug-disease prediction, we used a method based on a drug repositioning framework previously used for psychiatry traits [@doi:10.1038/nn.4618] where gene-trait associations are anticorrelated with expression profiles for drugs.
For the single-gene approach, we computed a drug-disease score by multiplying each S-PrediXcan set of results in tissue $t$, $\mathbf{M}^t$, with the transcriptional responses profiled in LINCS L1000 [@doi:10.1016/j.cell.2017.10.049], $\mathbf{L}^{c \times m}$ (for $c$ compounds): $\mathbf{D}^{t,k}=-1 \cdot \mathbf{M}^{t,k} \mathbf{L}^\top$, where $k$ refers to the number of most significant gene associations in $\mathbf{M}^t$ for each trait.
As suggested in [@doi:10.1038/nn.4618], $k$ could be either all genes or the top 50, 100, 250, and 500; then we average score ranks across all $k$ and obtain $\mathbf{D}^t$.
Finally, for each drug-disease pair, we took the maximum prediction score across all tissues: $\mathbf{D}_{ij} = \max \{ \mathbf{D}_{ij}^t \mid \forall t \}$.


The same procedure was used for the gene module-based approach, where we projected S-PrediXcan results into our latent representation, leading to $\hat{\mathbf{M}}^t$;
and also $\mathbf{L}$, leading to $\hat{\mathbf{L}}^{l \times c}$.
Finally, $\mathbf{D}^{t,k}=-1 \cdot \hat{\mathbf{M}}^{t,k} \hat{\mathbf{L}}^\top$, where in this case $k$ could be all LVs or the top 5, 10, 25 and 50 (since we have an order of magnitude less LVs than genes).


Since the gold standard of drug-disease medical indications used contained Disease Ontology IDs (DOID) [@doi:10.1093/nar/gky1032], we mapped PhenomeXcan traits to the Experimental Factor Ontology [@doi:10.1093/bioinformatics/btq099] using [@url:https://github.com/EBISPOT/EFO-UKB-mappings], and then to DOID.


### Consensus clustering of traits

We performed two preprocessing steps on the S-MultiXcan results before the cluster analysis procedure.
First, we combined results in $\mathbf{M}$ (S-MultiXcan) for traits that mapped to the same Experimental Factor Ontology (EFO) [@doi:10.1093/bioinformatics/btq099] term using the Stouffer's method: $\sum w_i M_{ij} / \sqrt{\sum w_i^2}$, where $w_i$ is a weight based on the GWAS sample size for trait $i$, and $M_{ij}$ is the $z$-score for gene $j$.
Second, we standardized all $z$-scores for each trait $i$ by their sum to reduce the effect of highly polygenic traits: $M_{ij} / \sum M_{ij}$.
Finally, we projected this data matrix using Equation @eq:proj, obtaining $\hat{\mathbf{M}}$ with $n$=3752 traits and $l$=987 LVs as the input of our clustering pipeline.


A partitioning of $\hat{\mathbf{M}}$ with $n$ traits into $k$ clusters is represented as a label vector $\pi \in \mathbb{N}^n$.
<!--  -->
Consensus clustering approaches consist of two steps:
1) the generation of an ensemble $\Pi$ with $r$ partitions of the dataset: $\Pi=\{\pi_1, \pi_2, \ldots, \pi_r\}$,
and 2) the combination of the ensemble into a consolidated solution defined as:

$$
\pi^* = \mathrm{arg}\,\underset{\hat{\pi}}{\max} Q(\{ \lvert \mathcal{L}^i \lvert \phi(\hat{\pi}_{\mathcal{L}^i}, \pi_{i \mathcal{L}^i}) \mid i \in \{1,\ldots,r\} \}),
$$ {#eq:consensus:obj_func}

where $\mathcal{L}^i$ is a set of data indices with known cluster labels for partition $i$,
$\phi\colon \mathbb{N}^n \times \mathbb{N}^n \to \mathbb{R}$ is a function that measures the similarity between two partitions,
and $Q$ is a measure of central tendency, such as the mean or median.
We used the adjusted Rand index (ARI) [@doi:10.1007/BF01908075] for $\phi$, and the median for $Q$.
<!--  -->
To obtain $\pi^*$, we define a consensus function $\Gamma\colon \mathbb{N}^{n \times r} \to \mathbb{N}^n$ with $\Pi$ as the input.
We used consensus functions based on the evidence accumulation clustering (EAC) paradigm [@doi:10.1109/TPAMI.2005.113], where $\Pi$ is first transformed into a distance matrix
<!--  -->
<!-- $\mathbf{D}_{ij} = \frac{d_{ij}}{r}$, -->
$\mathbf{D}_{ij} = d_{ij} / r$,
<!--  -->
where $d_{ij}$ is the number of times traits $i$ and $j$ were grouped in different clusters across all $r$ partitions in $\Pi$.
Then, $\Gamma$ can be any similarity-based clustering algorithm, which is applied on $\mathbf{D}$ to derive the final partition $\pi^*$.


For the ensemble generation step, we used different algorithms to create a highly diverse set of partitions (see Figure @fig:clustering:design) since diversity is an important property for ensembles [@doi:10.1016/j.ins.2016.04.027; @doi:10.1109/TPAMI.2011.84; @doi:10.1016/j.patcog.2014.04.005].
We used three data representations: the raw dataset, its projection into the top 50 principal components, and the embedding learned by UMAP [@arxiv:1802.03426] using 50 components.
<!--  -->
For each of these, we applied five clustering algorithms, covering a wide range of different assumptions on the data structure: $k$-means [@Arthur2007], spectral clustering [@Ng2001], a Gaussian mixture model (GMM), hierarchical clustering, and DBSCAN [@Ester1996].
<!--  -->
For $k$-means, spectral clustering and GMM, we specified a range of $k$ between 2 and $\sqrt{n} \approx 60$, and for each $k$ we generated five partitions using random seeds.
<!--  -->
For hierarchical clustering, for each $k$ we generated four partitions using four common linkage criteria: ward, complete, average and single.
<!--  -->
For DBSCAN, we combined different ranges for parameters $\epsilon$ (the maximum distance between two data points to be considered part of the same neighborhood) and *minPts* (the minimum number of data points in a neighborhood for a data point to be considered a core point).
Specifically, we used *minPts* values from 2 to 125, and for each data version, we determined a plausible range of $\epsilon$ values by observing the distribution of the mean distance of the *minPts*-nearest neighbors across all data points.
Since some combinations of *minPts* and $\epsilon$ might not produce a meaningful partition (for instance, when all points are detected as noisy or only one cluster is found), we resampled partitions generated by DBSCAN to ensure an equal representation in the ensemble.
<!--  -->
This procedure generated a final ensemble of 4428 partitions.


Finally, we used spectral clustering on $\mathbf{D}$ to derive the final consensus partitions.
$\mathbf{D}$ was first transformed into a similarity matrix by applying an RBF kernel $\mathrm{exp}(-\gamma \mathbf{D}^2)$ using four different values for $\gamma$ that we empirically determined to work best.
Thus for each $k$ between 2 and 60, we derived four consensus partitions and selected the one that maximized Equation @eq:consensus:obj_func.
<!--  -->
We further filtered this set of 59 solutions to keep only those with an ensemble agreement larger than the 75th percentile, leaving a total of 15 final consensus partitions shown in Figure @fig:clustering:tree.


### Cluster interpretation

We used a supervised learning approach to interpret clustering results by detecting which gene modules are the most important for clusters of traits.
For this, we used the highest resolution partition ($k$=29, although any could be used) to train a decision tree model using each of the clusters as labels and the projected data $\hat{\mathbf{M}}$ as the training samples.
For each $k$, we built a set of binary labels with the current cluster's traits as the positive class and the rest of the traits as the negative class.
Then, we selected the LV in the root node of the trained model only if its threshold was positive and larger than one standard deviation.
Next, we removed this LV from $\hat{\mathbf{M}}$ (regardless of being previously selected or not) and trained the model again.
We repeated this procedure 20 times to extract the top 20 LVs that better discriminate traits in a cluster from the rest.


### CRISPR-Cas9 screening

**Cell culture.**
HepG2 cells were obtained from ATCC (ATCC® HB-8065™), and maintained in Eagle’s Minimum Essential Medium with L-Glutamine (EMEM, Cat. 112-018-101, Quality Biology) supplemented with 10% Fetal Bovine Serum (FBS, Gibco, Cat.16000-044), and 1% Pen/Strep (Gibco, Cat.15140-122).
Cells were kept at 37oC in a humidity-controlled incubator with 5% CO2, and were maintained at a density not exceed more than 80% confluency.

**Genome-wide lentiviral pooled CRISPR-Cas9 library.**
3rd lentiviral generation, Broad GPP genome-wide Human Brunello CRISPR knockout Pooled library was provided by David Root and John Doench from Addgene (Cat. 73179-LV), and was used for HepG2 cell transduction.
It consists of 76,441 sgRNAs, targets 19,114 genes in the human genome with an average of 4 sgRNAs per gene.
Each 20nt sgRNA cassette was inserted into lentiCRIS-PRv2 backbone between U6 promoter and gRNA scaffold.
Through cell transduction, the lentiviral vectors which encode Cas9 were used to deliver the sgRNA cassette containing plasmids into cells during cell replication. 
Unsuccessful transduced cells were excluded through puromycin selection.

**Lentiviral titer determination.**
No-spin lentiviral transduction was utilized for the screen.
In a Collagen-I coated 6-wells plate, approximate 2.5 M cells were seeded each well in the presence of 8ug/ml polybrene (Millipore Sigma, Cat. TR-1003 G), and a different titrated virus volume (e.g., 0, 50, 100, 200, 250, and 400ul) was assigned to each well.
EMEM complete media was added to make the final volume of 1.24ml. 16-18hrs post transduction, virus/polybrene containing media was removed from each well.
Cells were washed twice with 1x DPBS, and replaced with fresh EMEM.
At 24h, cells in each well were trypsinized, diluted (e.g.,1:10), and seeded in pairs of wells of 6-well plates. At 60hr post transduction, cell media in each well was replaced with fresh EMEM. 2ug/ml of puromycin (Gibco, Cat. A1113803) was added to one well out of the pair. 2-5 days after puromycin selection, or the 0 virus well treated with puromycin had no survival of cells, cells in both wells with/without puromycin were collected and counted for viability.
Percentage of Infection (PI%) was obtained by comparing the cell numbers with/without puromycin selection within each pair.
By means of Poisson’s distribution theory, when transduction efficiency (PI%) is between 30-50%, which corresponding to an MOI (Multiplicity of Infection) of ~0.35-0.70. At MOI equal or close to 0.3, around 95% of infected cells are predicted to have only one copy of virus.
Therefore, a volume of virus (120ul) yielding 30-40% of transduction efficiency was chosen for further large-scale viral transduction.

**Lentiviral Transduction in HepG2 Using Brunello CRISPR Knockout Pooled Library.**
In order to achieve a coverage (representation) of at least 500 cells per sgRNA, and at an MOI between 0.3-0.4 to ensure 95% of infected cells get only one viral particle per cell, ~200M cells were initiated for the screen.
Transduction was carried out in the similar fashion as described above.
Briefly, 2.5M cells were seeded in each well of 14 6-well plates, along with 8ug/ml of polybrene.
Volume of 120ul of virus was added to each experimental well. 18hrs post transduction, virus/PB mix medium was removed, and cells in each well were collect-ed, counted, and pooled into T175 flasks.
At 60hr post transduction, 2ug/ml of puromycin was added to each flask.
Mediums were changed every 2 days with fresh EMEM, topped with 2ug/ml puromycin. 7 days after puromycin selection, cells were collected, pooled, counted, and replated.

**Fluorescent dye staining.** 9 days after puromycin selection, cells were assigned to 2 groups. 20-30M cells were collected as Unsorted Control.
Cell pellet was spun down at 500 x g for 5min at 4oC.
Dry pellet was kept at -80oC for further genomic DNA isolation.
The rest of the cells (approximately 200M) were kept in 100mm dishes, and stained with fluorescent dye (LipidSpotTM 488, Biotium, Cat. 70065-T).
In Brief, LipidSpot 488 was diluted to 1:100 with DPBS. 4ml of staining solution was used for each dish, and incubated at 37oC for 30min.
Cell images were captured through fluorescent microscope EVOS for GFP signal detection (Supplementary Figure @fig:sup:crispr:fig1).

**Fluorescence-activated cell sorting (FACS).**
Cells were immediately collected into 50ml tubes (From this point on, keep cells cold), and spin at 500 x g for 5min at 4oC.
After DPBS wash, cell pellets were resuspended with FACS Sorting Buffer (1x DPBS without Ca2+/Mg2+, 2.5mM EDTA, 25mM HEPES, 1% BSA.
Solution was filter sterilized, and kept at 4oC), pi-pet gently to make single cells.
Cell solution then filtered through cell strainer (Falcon, Cat. 352235), and were kept on ice protected from light.
Collected cells were sorted on FACSJazz. 100um nozzle was used for sorting. ~20% of each GFP-High and GFP-Low (Supplementary Figure @fig:sup:crispr:fig2) were collected into 15ml tubes.
After sorting, cells were immediately spun down. Pellets were kept in -80oC for further genomic DNA isolation.

**Genomic DNA isolation and verification.**
3 conditions of Genomic DNA (Un-Sorted Control, lentiV2 GFP-High, and lentiV2 GFP-Low) were extracted using QIAamp DNA Blood Mini Kit (Qiagen, Cat.51104), followed by UV Spectroscopy (Nanodrop) to access the quality and quantity of the gDNA.
Total 80-160ug of gDNA was isolated for each condition. sgRNA cassette and lentiviral specific transgene in isolated gDNA were verified through PCR (Supplementary Figure @fig:sup:crispr:fig3).

**Illumina libraries generation and sequencing.**
Fragment containing sgRNA cassette was amplified using P5 /P7 primers, as indicated in [@pmid:26780180] and primer sequences were adapted from Broad Institute protocol (Supplementary Figure @fig:sup:crispr:table1).
Stagger sequence (0-8nt) was included in P5, and 8bp uniquely barcoded sequence in P7.
Primers were synthesized through Integrated DNA Technologies (IDT), each primer was PAGE purified. 32 PCR reactions were set up for each condition.
Each 100ul PCR reaction consists of roughly 5ug of gDNA, 5ul of each 10uM P5 and P7. ExTaq DNA Polymerase (TaKaRa, Cat. RR001A) was used to amplify the amplicon.
PCR Thermal Cycler Parameters set as: Initial at 95oC for 1min; followed by 24 cycles of Denaturation at 94oC for 30 seconds, Annealing at 52.5oC for 30 seconds, Extension at 72oC for 30 seconds.
A final Elongation at 72oC for 10 minutes. 285bp-293bp PCR products were expected (Supplementary Figure @fig:sup:crispr:fig4 A).
PCR products within the same condition were pooled and purified using SPRIselect beads (Beckman Coulter, Cat. B23318).
Purified illumina libraries were quantitated on Qubit, and the quality of the library were analyzed on Bio-analyzer using High Sensitivity DNA Chip.
A single approximate 285bp peak was expected. (Supplementary Figure @fig:sup:crispr:fig4 B).
Final illumina library samples were sequenced on Nova-seq 6000.
Samples were pooled and loaded on a SP flow cell, along with 20% PhiX control v3 library spike-in.


### Code availability

The code to reproduce all the analyses in this work is available in [https://github.com/greenelab/phenoplier](https://github.com/greenelab/phenoplier)
